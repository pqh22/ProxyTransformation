from embodiedscan.registry import DATASETS
from mmengine import DefaultScope
from mmcv.transforms import BaseTransform, Compose
import numpy as np
import pickle
import os.path as osp

default_scope = DefaultScope.get_instance('embodiedscan', scope_name='embodiedscan')
dataset_type = 'EmbodiedScanDataset'
# data_root = '/cluster/nvme4b/embodied_data/'
data_root = '/cluster/home2/zjh/EmbodiedScan/data'
n_points = 100000
backend_args = None

class MultiViewPipelinePreprocess(BaseTransform):
    """Multiview data processing pipeline.

    The transform steps are as follows:

        1. Select frames.
        2. Re-ororganize the selected data structure.
        3. Apply transforms for each selected frame.
        4. Concatenate data to form a batch.

    Args:
        transforms (list[dict | callable]):
            The transforms to be applied to each select frame.
        n_images (int): Number of frames selected per scene.
        ordered (bool): Whether to put these frames in order.
            Defaults to False.
    """

    def __init__(self, transforms, n_images=None, ordered=True):
        super().__init__()
        self.transforms = Compose(transforms)
        self.n_images = n_images
        self.ordered = ordered

    def transform(self, results: dict):
        """Transform function.

        Args:
            results (dict): Result dict from loading pipeline.

        Returns:
            dict: output dict after transformation.
        """
        imgs = []
        img_paths = []
        points = []
        intrinsics = []
        extrinsics = []
        ids = np.arange(len(results['img_path']))
        

        # results['depth_img'] = depth_img
        # results['img'] = img
        # results['img_shape'] = img.shape[:2]
        # results['ori_shape'] = img.shape[:2]

        replace = True if self.n_images > len(ids) else False
        step = (len(ids) - 1) // (self.n_images - 1
                                    )  # TODO: BUG, fix from branch fbocc
        if step > 0:
            ids = ids[::step]
            # sometimes can not get the accurate n_images in this way
            # then take the first n_images one
            ids = ids[:self.n_images]
        else:
            ids = ids[:self.n_images]
    
        print('modified keys in results')
        new_dict = dict()
        for i in ids.tolist():
            _results = dict()
            _results['img_path'] = results['img_path'][i]
            if 'depth_img_path' in results:
                _results['depth_img_path'] = results['depth_img_path'][i]
                if isinstance(results['depth_cam2img'], list):
                    _results['depth_cam2img'] = results['depth_cam2img'][i]
                    _results['cam2img'] = results['depth2img']['intrinsic'][i]
                else:
                    _results['depth_cam2img'] = results['depth_cam2img']
                    _results['cam2img'] = results['cam2img']
                _results['depth_shift'] = results['depth_shift']
            _results = self.transforms(_results)
            
            if 'depth_shift' in _results:
                _results.pop('depth_shift')
            if 'img' in _results:
                imgs.append(_results['img'])
                img_paths.append(_results['img_path'])
            if 'points' in _results:
                points.append(_results['points'])
            if isinstance(results['depth2img']['intrinsic'], list):
                intrinsics.append(results['depth2img']['intrinsic'][i])
            else:
                intrinsics.append(results['depth2img']['intrinsic'])
            extrinsics.append(results['depth2img']['extrinsic'][i])
        for key in _results.keys():
            if key not in ['img', 'points', 'img_path']:
                print(key)
                new_dict[key] = _results[key]
                results[key] = _results[key]
        if len(imgs):
            print('img')
            print('img_path')
            results['img'] = imgs
            results['img_path'] = img_paths
            new_dict['img'] = imgs
            new_dict['img_path'] = img_paths
        if len(points):
            print('points')
            results['points'] = points
            new_dict['points'] = points
        if 'visible_instance_masks' in results:
            print('visible_instance_masks')
            results['visible_instance_masks'] = [
                results['visible_instance_masks'][i] for i in ids
            ]
            new_dict['visible_instance_masks'] = [
                results['visible_instance_masks'][i] for i in ids
            ]
        if 'visible_occupancy_masks' in results:
            print('visible_occupancy_masks')
            results['visible_occupancy_masks'] = [
                results['visible_occupancy_masks'][i] for i in ids
            ]
            new_dict['visible_occupancy_masks'] = [
                results['visible_occupancy_masks'][i] for i in ids
            ]
        print('depth2img-intrinsic')
        print('depth2img-extrinsic')
        results['depth2img']['intrinsic'] = intrinsics
        results['depth2img']['extrinsic'] = extrinsics
        new_dict['depth2img'] = results['depth2img']
        new_dict['scan_id'] = results['scan_id']
        return new_dict

class MultiViewPipelineLoadPreprocess(BaseTransform):
    """Multiview data processing pipeline.

    The transform steps are as follows:

        1. Select frames.
        2. Re-ororganize the selected data structure.
        3. Apply transforms for each selected frame.
        4. Concatenate data to form a batch.

    Args:
        transforms (list[dict | callable]):
            The transforms to be applied to each select frame.
        n_images (int): Number of frames selected per scene.
        ordered (bool): Whether to put these frames in order.
            Defaults to False.
    """

    def __init__(self, n_images=None, ordered=False, data_path=None):
        super().__init__()
        # self.transforms = Compose(transforms)
        self.n_images = n_images
        self.ordered = ordered
        self.data_path = data_path
    
    def get_scan_data(self, scan_id):
        scan_id = scan_id.split('/')
        scan_id = '_'.join(scan_id)
        return pickle.load(open(f'{self.data_path}/{scan_id}.pkl', 'rb'))

    def transform(self, results: dict):
        """Transform function.

        Args:
            results (dict): Result dict from loading pipeline.

        Returns:
            dict: output dict after transformation.
        """

        _results = self.get_scan_data(results['scan_id'])

        imgs = []
        img_paths = []
        points = []
        intrinsics = []
        extrinsics = []
        ids = np.arange(len(_results['img_path']))
        replace = True if self.n_images > len(ids) else False
        if self.ordered:
            step = (len(ids) - 1) // (self.n_images - 1
                                      )  # TODO: BUG, fix from branch fbocc
            if step > 0:
                ids = ids[::step]
                # sometimes can not get the accurate n_images in this way
                # then take the first n_images one
                ids = ids[:self.n_images]
            else:  # the number of images < pre-set n_images
                # randomly select n_images ids to enable batch-wise inference
                # In practice, can directly use the original ids to avoid
                # redundant computation
                ids = np.random.choice(ids, self.n_images, replace=replace)
        else:
            ids = np.random.choice(ids, self.n_images, replace=replace)

        for key in _results.keys():
            if key in ['img', 'points', 'img_path', 'visible_instance_masks', 'visible_occupancy_masks']:
                results[key] = [_results[key][i] for i in ids]
            elif key == 'depth2img':
                results['depth2img'] = dict()
                results['depth2img']['origin'] = _results['depth2img']['origin']
                results['depth2img']['extrinsic'] = [
                    _results['depth2img']['extrinsic'][i] for i in ids
                    ]
                results['depth2img']['intrinsic'] = [
                    _results['depth2img']['intrinsic'][i] for i in ids
                    ]
            else:
                results[key] = _results[key]
        return results

save_pipeline = [
    dict(type='LoadAnnotations3D'),
    MultiViewPipelinePreprocess(
         n_images=200,
         ordered=True,
         transforms=[
             dict(type='LoadImageFromFile', backend_args=backend_args),
             dict(type='LoadDepthFromFile', backend_args=backend_args),
             dict(type='ConvertRGBDToPoints', coord_type='CAMERA'),
             dict(type='PointSample', num_points=n_points // 10),
             dict(type='Resize', scale=(480, 480), keep_ratio=False)
         ]),
]
        
# train_pipeline = [
#     dict(type='LoadAnnotations3D'),
#     MultiViewPipelineLoadPreprocess(
#         n_images=20,
#         data_path='/cluster/home2/zjh/EmbodiedScan/data/newpreprocessed_data',
#     ),
#     # dict(type='MultiViewPipeline',
#     #      n_images=20,
#     #      transforms=[
#     #          dict(type='LoadImageFromFile', backend_args=backend_args),
#     #          dict(type='LoadDepthFromFile', backend_args=backend_args),
#     #          dict(type='ConvertRGBDToPoints', coord_type='CAMERA'),
#     #          dict(type='PointSample', num_points=n_points // 10),
#     #          dict(type='Resize', scale=(480, 480), keep_ratio=False)
#     #      ]),
#     # dict(type='AggregateMultiViewPoints', coord_type='DEPTH'),
#     # dict(type='PointSample', num_points=n_points),
#     # dict(type='GlobalRotScaleTrans',
#     #      rot_range=[-0.087266, 0.087266],
#     #      scale_ratio_range=[.9, 1.1],
#     #      translation_std=[.1, .1, .1],
#     #      shift_height=False),
#     # dict(type='Pack3DDetInputs',
#     #      keys=['img', 'points', 'gt_bboxes_3d', 'gt_labels_3d'])
# ]


# class timeit(BaseTransform):
#     def __init__(self, function_name=None):
#         self.function_name = function_name
#     def transform(self, results):
#         import time
#         if results.get('start_time', None) is None:
#             results['start_time'] = time.time()
#             results['init_time'] = time.time()
#         else:
#             self.end = time.time()
#             print(f"Function:{self.function_name} Time taken: {self.end - results['start_time']}")
#             if self.function_name == 'LoadAnnotations3D':
#                 results['MVP_start_time'] = time.time()
#             elif self.function_name == 'MultiViewPipeline':
#                 print(f"Function:{self.function_name} Time taken: {self.end - results['MVP_start_time']}")
#             results['start_time'] = time.time()
#         return results


# data_pipeline = [
#     # timeit('Init'),
#     dict(type='LoadAnnotations3D', _scope_='embodiedscan'),
#     # timeit('LoadAnnotations3D'),
#     dict(type='MultiViewPipeline',
#          n_images=20,
#          ordered=True,
#          transforms=[
#             #  timeit('MVP_Prepare'),
#              dict(type='LoadImageFromFile', backend_args=backend_args),
#             #  timeit('LoadImageFromFile'),
#              dict(type='LoadDepthFromFile', backend_args=backend_args),
#             #  timeit('LoadDepthFromFile'),
#              dict(type='ConvertRGBDToPoints', coord_type='CAMERA'),
#             #  timeit('ConvertRGBDToPoints'),
#              dict(type='PointSample', num_points=n_points // 10),
#             #  timeit('PointSample'),
#              dict(type='Resize', scale=(480, 480), keep_ratio=False),
#             #  timeit('Resize'),
#          ]),
#     # timeit('MultiViewPipeline'),
#     # dict(type='AggregateMultiViewPoints', coord_type='DEPTH'),
#     # timeit('AggregateMultiViewPoints'),
#     # dict(type='SavingPreprocessData', save_dir=data_root+'/preprocessed_data_debug'),
# ]

metainfo = dict(classes='all_from_info')
dataset_type = 'MultiView3DGroundingDataset'
dataset_config=dict(type=dataset_type,
            data_root=data_root,
            ann_file='embodiedscan_infos_train_referit_scanrefer.pkl',
            vg_file='embodiedscan_train_mini_vg_4samples_fordebug.json',
            metainfo=metainfo,
            pipeline=save_pipeline,
            test_mode=False,
            filter_empty_gt=True,
            box_type_3d='Euler-Depth')
            
dataset = DATASETS.build(dataset_config)

from tqdm import tqdm
import pdb; pdb.set_trace()
save_path = '/cluster/nvme4b/zjh/preprocessed_data_referit'
import time
start = time.time()

for idx in tqdm(range(len(dataset))):
    data = dataset[idx]
    # print(time.time()-start, f'img_count:{data}')
    # start = time.time()
    scan_id = data['scan_id'].split('/')
    scan_id = '_'.join(scan_id)
    save_file = osp.join(save_path, f'{scan_id}.pkl')
    if os.path.exists(save_file):
        continue
    with open(save_file, 'wb') as f:
        pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)
